{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher20.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "ROLLOUT_LENGTH = 250 \n",
    "DISCOUNT = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "OPTIMIZATION_EPOCHS = 10\n",
    "MINI_BATCH_SIZE = 64\n",
    "PPO_RATIO_CLIP = 0.1\n",
    "GRADIENT_CLIP = 0.75\n",
    "HIDDEN_LAYERS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-party Code\n",
    "These are helper routines copied or adapted from other projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to Shangtong Zhang \n",
    "# https://github.com/ShangtongZhang/DeepRL\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "def to_np(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "def tensor(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = torch.tensor(x, device=DEVICE, dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]\n",
    "\n",
    "class MeanStdNormalizer():\n",
    "    def __init__(self):\n",
    "        self.rms = None\n",
    "        self.clip = 10.0\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.rms is None:\n",
    "            self.rms = RunningMeanStd(shape=(1, ) + x.shape[1:])\n",
    "        self.rms.update(x)\n",
    "        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n",
    "                       -self.clip, self.clip)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to OpenAI.\n",
    "\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/running_mean_std.py    \n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)    \n",
    "        \n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Agent\n",
    "\n",
    "The master agent implements the PPO algorithm and can use multiple sub-agents for the purpose of samlping trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class SubNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, output_size, seed):\n",
    "        super(SubNetwork, self).__init__()\n",
    "        dims = (input_size,) + hidden_units        \n",
    "        self.layers = nn.ModuleList([layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.feature_dim = dims[-1]\n",
    "        self.output_layer = layer_init(nn.Linear(self.feature_dim, output_size), 1e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.tanh(layer(x))\n",
    "        x = self.output_layer(x)    \n",
    "        return x    \n",
    "            \n",
    "class ActorAndCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        super(ActorAndCritic, self).__init__()\n",
    "        self.seed = random.seed(seed)\n",
    "        self.actor = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), action_size, seed)\n",
    "        self.critic = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), 1, seed)\n",
    "        self.std = nn.Parameter(torch.zeros(action_size))\n",
    "        #self.to(Config.DEVICE)\n",
    "        \n",
    "    def forward(self, obs, action=None):\n",
    "        obs = tensor(obs)\n",
    "        a = self.actor(obs)\n",
    "        v = self.critic(obs)\n",
    "        mean = F.tanh(a)\n",
    "        dist = torch.distributions.Normal(mean, F.softplus(self.std))\n",
    "        return (v, dist)\n",
    "        \n",
    "class Rollout():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stored values\n",
    "        self.actions = []\n",
    "        self.log_prob_actions = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.episode_not_dones = []\n",
    "        self.states = []\n",
    "        # Calculated values\n",
    "        self.returns = [0.0] * ROLLOUT_LENGTH\n",
    "        self.advantages = [0.0] * ROLLOUT_LENGTH\n",
    "        \n",
    "    def save_prediction(self, actions, log_prob_actions, values):\n",
    "        self.actions.append(actions)\n",
    "        self.log_prob_actions.append(log_prob_actions)\n",
    "        self.values.append(values)\n",
    "\n",
    "    def save_consequences(self, rewards, episode_not_dones, states):\n",
    "        self.rewards.append(rewards)\n",
    "        self.episode_not_dones.append(episode_not_dones)\n",
    "        self.states.append(states)\n",
    "        \n",
    "    def calculate_returns_and_advantages(self, final_reward):\n",
    "        self.rewards.append(None)\n",
    "        self.episode_not_dones.append(None)\n",
    "        self.calculate_future_returns(final_reward)\n",
    "        self.estimate_advantages()\n",
    "\n",
    "    def calculate_future_returns(self, returns):\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            returns = self.rewards[i] + DISCOUNT * self.episode_not_dones[i] * returns\n",
    "            self.returns[i] = returns.detach() \n",
    "\n",
    "    def estimate_advantages(self):\n",
    "        advantages = tensor(np.zeros((num_agents, 1)))\n",
    "        # Go backwards through rollout steps and calculate advantages for each state action pair\n",
    "        # Use GAE for PPO. (Schulman, Moritz, Levine et al. 2016)\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            td = self.rewards[i] + (DISCOUNT * self.episode_not_dones[i] * self.values[i + 1]) - self.values[i]\n",
    "            advantages = advantages * GAE_LAMBDA * DISCOUNT * self.episode_not_dones[i] + td\n",
    "            self.advantages[i] = advantages.detach()               \n",
    "\n",
    "    def stack_tensor(self, some_list):\n",
    "        return torch.cat(some_list[:ROLLOUT_LENGTH], dim=0)\n",
    "            \n",
    "    def get_sample_data(self):\n",
    "        states = self.stack_tensor(self.states)\n",
    "        actions = self.stack_tensor(self.actions) \n",
    "        log_prob_actions = self.stack_tensor(self.log_prob_actions)\n",
    "        returns = self.stack_tensor(self.returns)\n",
    "        # Normalize advantages\n",
    "        advantages = self.stack_tensor(self.advantages)\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()        \n",
    "        return (states, actions, log_prob_actions, returns, advantages)\n",
    "    \n",
    "class MasterAgent():   \n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        self.network = ActorAndCritic(num_agents, state_size, action_size, seed)\n",
    "        self.first_states = True\n",
    "        self.total_steps = 0\n",
    "        self.state_normalizer = MeanStdNormalizer()\n",
    "        \n",
    "    def evaluate_actions_against_states(self, states, actions):\n",
    "        value, action_distribution = self.network(states, actions)\n",
    "        log_prob = self.get_log_prob(action_distribution, actions)\n",
    "        return (log_prob, value)\n",
    "    \n",
    "    def get_log_prob(self, action_distribution, actions):\n",
    "        return action_distribution.log_prob(actions).sum(-1).unsqueeze(-1)\n",
    "    \n",
    "    def get_prediction(self, states):\n",
    "        if self.first_states:\n",
    "            self.states = states\n",
    "            self.first_states = False\n",
    "        #self.latest_actions, self.latest_log_prob, self.latest_values = self.get_prediction_from_states(self.states)\n",
    "        self.latest_values, action_distribution = self.network(self.states)\n",
    "        self.latest_actions = action_distribution.sample()\n",
    "        self.latest_log_prob = self.get_log_prob(action_distribution, self.latest_actions)\n",
    "        return self.latest_actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        rewards = np.asarray(rewards)\n",
    "        next_states = self.state_normalizer(next_states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "        dones = np.asarray(dones).astype(int)\n",
    "        rewards = tensor(rewards).unsqueeze(-1)\n",
    "        episode_not_dones = tensor(1 - dones).unsqueeze(-1)\n",
    "        states = tensor(self.states)        \n",
    "        self.rollout.save_consequences(rewards, episode_not_dones, states)\n",
    "\n",
    "        self.states = next_states\n",
    "                \n",
    "    def start_rollout(self):\n",
    "        self.rollout = Rollout()\n",
    "            \n",
    "    def process_rollout(self, states):\n",
    "        self.save_final_results(states)\n",
    "        self.rollout.calculate_returns_and_advantages(self.latest_values.detach())\n",
    "        self.optimize()\n",
    "        self.first_states = True\n",
    "        \n",
    "    def save_final_results(self, states):    \n",
    "        self.get_prediction(states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "   \n",
    "    def save_weights(self):\n",
    "        print(\"======== Saving weights ==========\")\n",
    "        torch.save(self.network.state_dict(), \"trained_weights.pth\")\n",
    "\n",
    "    def optimize(self):\n",
    "        # Now use tensors for 's', 'a', 'log_pi_a', 'ret', 'adv' for training\n",
    "        # states, actions, log prob actions, returns, advantages (1 row / timestep, 1 column per worker)\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = self.rollout.get_sample_data()\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        optimizer = torch.optim.Adam(self.network.parameters(), 3e-4, eps=1e-5)\n",
    "        for i in range(OPTIMIZATION_EPOCHS):\n",
    "            number_timesteps = states.size(0)\n",
    "            timesteps_to_sample = random_sample(np.arange(number_timesteps), MINI_BATCH_SIZE) \n",
    "            for timestep in timesteps_to_sample:\n",
    "                t = tensor(timestep).long()\n",
    "                # Get data for all workers from sampled timestep \n",
    "                sampled_states = states[t]\n",
    "                sampled_actions = actions[t]\n",
    "                sampled_log_probs_old = log_probs_old[t]\n",
    "                sampled_returns = returns[t]\n",
    "                sampled_advantages = advantages[t]\n",
    "                self.optimize_with_sampled_worker_data(optimizer, sampled_states,\n",
    "                                                                  sampled_actions,\n",
    "                                                                  sampled_log_probs_old,\n",
    "                                                                  sampled_returns,\n",
    "                                                                  sampled_advantages)\n",
    "        steps = ROLLOUT_LENGTH * num_agents\n",
    "        # Total steps used to train network\n",
    "        self.total_steps += steps\n",
    "        \n",
    "    def optimize_with_sampled_worker_data(self, optimizer, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages):\n",
    "        # Get log_prob(actions) and value given states\n",
    "        # Pass in states for all workers x batch_size.\n",
    "        log_prob_action, value = self.evaluate_actions_against_states(sampled_states, sampled_actions)\n",
    "        \n",
    "        policy_loss = self.get_policy_loss(log_prob_action, sampled_log_probs_old, sampled_advantages)\n",
    "        value_loss = self.get_value_loss(value, sampled_returns)\n",
    "        \n",
    "        # Do the actual optimization\n",
    "        optimizer.zero_grad()\n",
    "        # Overall loss function for training both networks at once. Get gradients on weights.\n",
    "        (policy_loss + value_loss).backward()\n",
    "        # Clip weight gradients \n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP) \n",
    "        # Run actual optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    def get_policy_loss(self, log_prob_action, sampled_log_probs_old, sampled_advantages):\n",
    "        # This is the core of PPO\n",
    "        # ratio = new prob / old prob for all workers\n",
    "        ratio = (log_prob_action - sampled_log_probs_old).exp() \n",
    "        # Clip loss on the upside\n",
    "        clamped_ratio = ratio.clamp(1.0 - PPO_RATIO_CLIP, 1.0 + PPO_RATIO_CLIP)\n",
    "        obj = ratio * sampled_advantages\n",
    "        obj_clipped = clamped_ratio * sampled_advantages\n",
    "        policy_loss = -torch.min(obj, obj_clipped).mean() \n",
    "        return policy_loss\n",
    "    \n",
    "    def get_value_loss(self, value, sampled_returns):\n",
    "        # Mean squared error\n",
    "        value_loss = 0.5 * (sampled_returns - value).pow(2).mean()\n",
    "        return value_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The TrainingSession class trains the agent while monitoring the progress of the episodes. It can also simply run an episode with the previously trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSession():\n",
    "    \n",
    "    def __init__(self, num_workers):\n",
    "        self.num_workers = num_workers\n",
    "        self.online_rewards = np.zeros(num_workers)\n",
    "        self.mean_last_100 = 0\n",
    "        self.mean_scores = []\n",
    "\n",
    "    def train_ppo(self, agent, target_average_score, max_episodes=300):\n",
    "        print(\"Attempting to reach 100 episode trailing average of {:.2f} in under {} episodes.\".format(target_average_score, max_episodes))\n",
    "        print(\"Rollout length: %s\" % ROLLOUT_LENGTH)\n",
    "        print(\"GRADIENT_CLIP %s\" % GRADIENT_CLIP)\n",
    "        print(\"PPO_RATIO_CLIP %s\" % PPO_RATIO_CLIP)\n",
    "        print(\"GAE_LAMBDA %s\" % GAE_LAMBDA)\n",
    "        #agent.save_weights()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment   \n",
    "        self.num_episodes = 0\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.episode_scores = [list() for i in range(num_agents)]\n",
    "        self.episodes_finished = 0\n",
    "        self.target_average_score = target_average_score\n",
    "        while True:\n",
    "            mean_last_100 = self.run_rollout(agent, env_info)\n",
    "            agent.process_rollout(states)\n",
    "            \n",
    "            #print(\"\\rEpisode {}\\tLast 100: {:.2f}\".format(self.num_episodes, mean_last_100))\n",
    "            if mean_last_100 > target_average_score:\n",
    "                print(\"Reached target! mean_last_100 %s\" % mean_last_100)\n",
    "                agent.save_weights() \n",
    "                break\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Failed to reach target in {} episodes.\".format(self.num_episodes))\n",
    "                break\n",
    "        return self.mean_scores \n",
    "    \n",
    "    def get_actions_from_policy(self, states):\n",
    "        actions = agent.get_prediction(states)          # Run the policy \n",
    "        actions = to_np(actions)                        # Extract actions\n",
    "        actions = np.clip(actions, -1, 1)               # all actions between -1 and 1\n",
    "        return actions\n",
    "    \n",
    "    def run_ppo(self, agent, max_episodes=5):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.num_episodes = 0\n",
    "        while True:\n",
    "            mean_score_over_agents = self.run_rollout(agent, env_info)\n",
    "            self.num_episodes += 1\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Policy failed to reach target in %s\" % max_episodes)\n",
    "                break\n",
    "    \n",
    "    def step_environment(self, env, actions):\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment        \n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished for each agent\n",
    "        return (next_states, rewards, dones)\n",
    "                                \n",
    "    def run_episode(self, env_info):\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        while True:\n",
    "            actions = self.get_actions_from_policy(states)\n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "            \n",
    "    def run_rollout(self, agent, env_info):\n",
    "        #print(\"Run rollout\")\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        for t in range(ROLLOUT_LENGTH):\n",
    "            actions = self.get_actions_from_policy(states)            \n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "        \n",
    "            # Save rewards\n",
    "            self.online_rewards += rewards                          # Accumulate ongoing (un-normalized) rewards for each agent\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:                                       # For a worker whose episode is done...\n",
    "                    #print(\"Worker %s finished at timestep %s\" % (i, t))\n",
    "                    self.end_episode(i, self.online_rewards[i])\n",
    "                    self.online_rewards[i] = 0                 # Reset accumulated reward for next episode\n",
    "                    self.mean_last_100 = np.mean(self.last_100_scores)\n",
    "                    if self.mean_last_100 > self.target_average_score:\n",
    "                        print(\"Breaking\")\n",
    "                        break\n",
    "            #print(\"%s step\" % t)            \n",
    "            agent.step(states, actions, rewards, next_states, dones) # Teach the agent \n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "\n",
    "        #print(\"Steps in rollout: {}\".format(t+1))        \n",
    "        return self.mean_last_100    \n",
    "\n",
    "    def end_episode(self, agent_index, score):\n",
    "        self.episode_scores[agent_index].append(score)   # Save the reward they accumulated in the episode\n",
    "        self.episodes_finished +=1\n",
    "        if (self.episodes_finished % num_agents) == 0:\n",
    "            self.num_episodes += 1\n",
    "            total_over_agents = 0\n",
    "            for i in range(num_agents):\n",
    "                total_over_agents += self.episode_scores[i][-1]\n",
    "            mean_score_over_agents = total_over_agents / num_agents    \n",
    "            self.last_100_scores.append(mean_score_over_agents)\n",
    "            self.mean_scores.append(mean_score_over_agents)\n",
    "            print(\"Finished %s episodes (%s cycles). mean_score_over_agents %s trailing %s\" % (self.num_episodes, (self.episodes_finished/num_agents), mean_score_over_agents, np.mean(self.last_100_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Policy\n",
    "\n",
    "Running the code in the cell below trains the policy, attempting to reach the target treailing avewrage value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reach 100 episode trailing average of 30.00 in under 300 episodes.\n",
      "Rollout length: 250\n",
      "GRADIENT_CLIP 0.75\n",
      "PPO_RATIO_CLIP 0.1\n",
      "GAE_LAMBDA 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/drlnd/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda2/envs/drlnd/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 episodes (1.0 cycles). mean_score_over_agents 0.20299999546259642 trailing 0.20299999546259642\n",
      "Finished 2 episodes (2.0 cycles). mean_score_over_agents 0.26699999403208496 trailing 0.23499999474734068\n",
      "Finished 3 episodes (3.0 cycles). mean_score_over_agents 0.48799998909235 trailing 0.3193333261956771\n",
      "Finished 4 episodes (4.0 cycles). mean_score_over_agents 0.5539999876171351 trailing 0.3779999915510416\n",
      "Finished 5 episodes (5.0 cycles). mean_score_over_agents 0.5954999866895377 trailing 0.4214999905787408\n",
      "Finished 6 episodes (6.0 cycles). mean_score_over_agents 0.8284999814815819 trailing 0.48933332239588107\n",
      "Finished 7 episodes (7.0 cycles). mean_score_over_agents 1.1949999732896686 trailing 0.5901428439521365\n",
      "Finished 8 episodes (8.0 cycles). mean_score_over_agents 1.2689999716356397 trailing 0.6749999849125743\n",
      "Finished 9 episodes (9.0 cycles). mean_score_over_agents 1.5454999654553832 trailing 0.7717222049728863\n",
      "Finished 10 episodes (10.0 cycles). mean_score_over_agents 1.5639999650418759 trailing 0.8509499809797854\n",
      "Finished 11 episodes (11.0 cycles). mean_score_over_agents 1.8404999588616193 trailing 0.9409090698781338\n",
      "Finished 12 episodes (12.0 cycles). mean_score_over_agents 1.5669999649748205 trailing 0.9930833111361911\n",
      "Finished 13 episodes (13.0 cycles). mean_score_over_agents 2.0579999540001155 trailing 1.0749999759718776\n",
      "Finished 14 episodes (14.0 cycles). mean_score_over_agents 2.049499954190105 trailing 1.1446071172731795\n",
      "Finished 15 episodes (15.0 cycles). mean_score_over_agents 2.246499949786812 trailing 1.218066639440755\n",
      "Finished 16 episodes (16.0 cycles). mean_score_over_agents 2.0344999545253812 trailing 1.2690937216335443\n",
      "Finished 17 episodes (17.0 cycles). mean_score_over_agents 2.5889999421313403 trailing 1.3467352640157677\n",
      "Finished 18 episodes (18.0 cycles). mean_score_over_agents 2.7749999379739165 trailing 1.426083301457887\n",
      "Finished 19 episodes (19.0 cycles). mean_score_over_agents 2.7164999392814932 trailing 1.4939999666064978\n",
      "Finished 20 episodes (20.0 cycles). mean_score_over_agents 2.6079999417066575 trailing 1.549699965361506\n",
      "Finished 21 episodes (21.0 cycles). mean_score_over_agents 2.2284999501891436 trailing 1.5820237741628222\n",
      "Finished 22 episodes (22.0 cycles). mean_score_over_agents 3.341999925300479 trailing 1.6620226901236246\n",
      "Finished 23 episodes (23.0 cycles). mean_score_over_agents 2.706999939493835 trailing 1.7074564835745034\n",
      "Finished 24 episodes (24.0 cycles). mean_score_over_agents 3.182999928854406 trailing 1.7689374604611656\n",
      "Finished 25 episodes (25.0 cycles). mean_score_over_agents 2.9489999340847133 trailing 1.8161399594061074\n",
      "Finished 26 episodes (26.0 cycles). mean_score_over_agents 2.7949999375268817 trailing 1.8537884201030603\n",
      "Finished 27 episodes (27.0 cycles). mean_score_over_agents 3.4634999225847425 trailing 1.913407364639419\n",
      "Finished 28 episodes (28.0 cycles). mean_score_over_agents 3.6014999195002018 trailing 1.9736963844558755\n",
      "Finished 29 episodes (29.0 cycles). mean_score_over_agents 3.5999999195337296 trailing 2.0297758166999396\n",
      "Finished 30 episodes (30.0 cycles). mean_score_over_agents 4.294499904010445 trailing 2.1052666196102896\n",
      "Finished 31 episodes (31.0 cycles). mean_score_over_agents 4.082999908737838 trailing 2.169064467646662\n",
      "Finished 32 episodes (32.0 cycles). mean_score_over_agents 4.66199989579618 trailing 2.2469686997763345\n",
      "Finished 33 episodes (33.0 cycles). mean_score_over_agents 4.741499894019216 trailing 2.322560554147331\n",
      "Finished 34 episodes (34.0 cycles). mean_score_over_agents 4.579999897629023 trailing 2.388955828955616\n",
      "Finished 35 episodes (35.0 cycles). mean_score_over_agents 4.848999891616404 trailing 2.4592428021744954\n",
      "Finished 36 episodes (36.0 cycles). mean_score_over_agents 4.955999889224768 trailing 2.52859716570367\n",
      "Finished 37 episodes (37.0 cycles). mean_score_over_agents 6.187999861687421 trailing 2.627499941270798\n",
      "Finished 38 episodes (38.0 cycles). mean_score_over_agents 6.0279998652637 trailing 2.716986781375874\n",
      "Finished 39 episodes (39.0 cycles). mean_score_over_agents 5.998999865911901 trailing 2.8011409630306443\n",
      "Finished 40 episodes (40.0 cycles). mean_score_over_agents 7.329499836172909 trailing 2.9143499348592012\n",
      "Finished 41 episodes (41.0 cycles). mean_score_over_agents 6.720999849773944 trailing 3.0071950547351705\n",
      "Finished 42 episodes (42.0 cycles). mean_score_over_agents 6.998499843571335 trailing 3.1022261211360314\n",
      "Finished 43 episodes (43.0 cycles). mean_score_over_agents 7.900499823410064 trailing 3.213813881654032\n",
      "Finished 44 episodes (44.0 cycles). mean_score_over_agents 7.756499826628715 trailing 3.3170567440398204\n",
      "Finished 45 episodes (45.0 cycles). mean_score_over_agents 6.878999846242368 trailing 3.3962110351998773\n",
      "Finished 46 episodes (46.0 cycles). mean_score_over_agents 8.813999802991749 trailing 3.5139890518910053\n",
      "Finished 47 episodes (47.0 cycles). mean_score_over_agents 9.565999786183237 trailing 3.64275523772701\n",
      "Finished 48 episodes (48.0 cycles). mean_score_over_agents 11.052499752957374 trailing 3.7971249151276427\n",
      "Finished 49 episodes (49.0 cycles). mean_score_over_agents 9.828499780315905 trailing 3.920214198090668\n",
      "Finished 50 episodes (50.0 cycles). mean_score_over_agents 11.883999734371901 trailing 4.079489908816293\n",
      "Finished 51 episodes (51.0 cycles). mean_score_over_agents 10.355499768536538 trailing 4.202548925673552\n",
      "Finished 52 episodes (52.0 cycles). mean_score_over_agents 10.89949975637719 trailing 4.331336441648623\n",
      "Finished 53 episodes (53.0 cycles). mean_score_over_agents 13.566499696765096 trailing 4.505584804952707\n",
      "Finished 54 episodes (54.0 cycles). mean_score_over_agents 13.864499690104276 trailing 4.678898043566625\n",
      "Finished 55 episodes (55.0 cycles). mean_score_over_agents 15.192499660421163 trailing 4.870054436600344\n",
      "Finished 56 episodes (56.0 cycles). mean_score_over_agents 14.549999674782157 trailing 5.042910601567876\n",
      "Finished 57 episodes (57.0 cycles). mean_score_over_agents 16.439999632537365 trailing 5.242859531935762\n",
      "Finished 58 episodes (58.0 cycles). mean_score_over_agents 15.990499642584473 trailing 5.428163671774533\n",
      "Finished 59 episodes (59.0 cycles). mean_score_over_agents 13.690499693993479 trailing 5.568203265371465\n",
      "Finished 60 episodes (60.0 cycles). mean_score_over_agents 14.822499668691307 trailing 5.722441538760129\n",
      "Finished 61 episodes (61.0 cycles). mean_score_over_agents 15.049499663617462 trailing 5.8753441309709045\n",
      "Finished 62 episodes (62.0 cycles). mean_score_over_agents 15.679499649535865 trailing 6.03347567159292\n",
      "Finished 63 episodes (63.0 cycles). mean_score_over_agents 20.43899954315275 trailing 6.262134780665298\n",
      "Finished 64 episodes (64.0 cycles). mean_score_over_agents 16.86199962310493 trailing 6.427757668828417\n",
      "Finished 65 episodes (65.0 cycles). mean_score_over_agents 20.122499550227076 trailing 6.63844600546532\n",
      "Finished 66 episodes (66.0 cycles). mean_score_over_agents 17.361999611929058 trailing 6.80092409041174\n",
      "Finished 67 episodes (67.0 cycles). mean_score_over_agents 19.534999563358724 trailing 6.9909849183661725\n",
      "Finished 68 episodes (68.0 cycles). mean_score_over_agents 18.8734995781444 trailing 7.16572778100997\n",
      "Finished 69 episodes (69.0 cycles). mean_score_over_agents 20.5539995405823 trailing 7.359760705061743\n",
      "Finished 70 episodes (70.0 cycles). mean_score_over_agents 20.844499534089117 trailing 7.552399831190706\n",
      "Finished 71 episodes (71.0 cycles). mean_score_over_agents 20.527999541163446 trailing 7.73515475668328\n",
      "Finished 72 episodes (72.0 cycles). mean_score_over_agents 21.432499520946294 trailing 7.925395656186932\n",
      "Finished 73 episodes (73.0 cycles). mean_score_over_agents 21.02499953005463 trailing 8.10484228459608\n",
      "Finished 74 episodes (74.0 cycles). mean_score_over_agents 23.559499473404138 trailing 8.313689003363756\n",
      "Finished 75 episodes (75.0 cycles). mean_score_over_agents 20.637999538704754 trailing 8.47801314383497\n",
      "Finished 76 episodes (76.0 cycles). mean_score_over_agents 23.81749946763739 trailing 8.67984849020079\n",
      "Finished 77 episodes (77.0 cycles). mean_score_over_agents 19.181499571260066 trailing 8.816233569175585\n",
      "Finished 78 episodes (78.0 cycles). mean_score_over_agents 24.58749945042655 trailing 9.01842928560188\n",
      "Finished 79 episodes (79.0 cycles). mean_score_over_agents 22.60399949476123 trailing 9.190398528755795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 80 episodes (80.0 cycles). mean_score_over_agents 23.086999483965336 trailing 9.364106040695916\n",
      "Finished 81 episodes (81.0 cycles). mean_score_over_agents 21.47549951998517 trailing 9.513629416983438\n",
      "Finished 82 episodes (82.0 cycles). mean_score_over_agents 22.02999950759113 trailing 9.666268076624995\n",
      "Finished 83 episodes (83.0 cycles). mean_score_over_agents 22.800999490357935 trailing 9.824517852694067\n",
      "Finished 84 episodes (84.0 cycles). mean_score_over_agents 23.820499467570336 trailing 9.991136681442594\n",
      "Finished 85 episodes (85.0 cycles). mean_score_over_agents 25.357999433204533 trailing 10.171923302051558\n",
      "Finished 86 episodes (86.0 cycles). mean_score_over_agents 23.29949947921559 trailing 10.324569536669744\n",
      "Finished 87 episodes (87.0 cycles). mean_score_over_agents 22.56449949564412 trailing 10.465258386772899\n",
      "Finished 88 episodes (88.0 cycles). mean_score_over_agents 23.64949947139248 trailing 10.615079308189028\n",
      "Finished 89 episodes (89.0 cycles). mean_score_over_agents 25.410499432031067 trailing 10.781319983737815\n",
      "Finished 90 episodes (90.0 cycles). mean_score_over_agents 24.332499456126243 trailing 10.93188864454213\n",
      "Finished 91 episodes (91.0 cycles). mean_score_over_agents 26.537499406840652 trailing 11.103378872699258\n",
      "Finished 92 episodes (92.0 cycles). mean_score_over_agents 26.843999399989844 trailing 11.27447257408285\n",
      "Finished 93 episodes (93.0 cycles). mean_score_over_agents 27.549999384209514 trailing 11.449478238707869\n",
      "Finished 94 episodes (94.0 cycles). mean_score_over_agents 25.99599941894412 trailing 11.60422846402953\n",
      "Finished 95 episodes (95.0 cycles). mean_score_over_agents 24.336499456036837 trailing 11.738252369208555\n",
      "Finished 96 episodes (96.0 cycles). mean_score_over_agents 28.009499373938887 trailing 11.907744525507828\n",
      "Finished 97 episodes (97.0 cycles). mean_score_over_agents 25.35199943333864 trailing 12.04634509156794\n",
      "Finished 98 episodes (98.0 cycles). mean_score_over_agents 25.77949942378327 trailing 12.18647931944769\n",
      "Finished 99 episodes (99.0 cycles). mean_score_over_agents 27.32949938913807 trailing 12.339439118131432\n",
      "Finished 100 episodes (100.0 cycles). mean_score_over_agents 25.10799943879247 trailing 12.467124721338042\n",
      "Finished 101 episodes (101.0 cycles). mean_score_over_agents 26.133499415870755 trailing 12.726429715542125\n",
      "Finished 102 episodes (102.0 cycles). mean_score_over_agents 27.529499384667723 trailing 12.99905470944848\n",
      "Finished 103 episodes (103.0 cycles). mean_score_over_agents 28.816999355889855 trailing 13.282344703116456\n",
      "Finished 104 episodes (104.0 cycles). mean_score_over_agents 25.330499433819206 trailing 13.530109697578474\n",
      "Finished 105 episodes (105.0 cycles). mean_score_over_agents 27.074499394837765 trailing 13.794899691659959\n",
      "Finished 106 episodes (106.0 cycles). mean_score_over_agents 27.51099938508123 trailing 14.061724685695957\n",
      "Finished 107 episodes (107.0 cycles). mean_score_over_agents 28.971499352436513 trailing 14.339489679487421\n",
      "Finished 108 episodes (108.0 cycles). mean_score_over_agents 26.173499414976686 trailing 14.588534673920835\n",
      "Finished 109 episodes (109.0 cycles). mean_score_over_agents 26.112499416340142 trailing 14.834204668429681\n",
      "Finished 110 episodes (110.0 cycles). mean_score_over_agents 27.059999395161867 trailing 15.089164662730882\n",
      "Finished 111 episodes (111.0 cycles). mean_score_over_agents 27.620499382633717 trailing 15.3469646569686\n",
      "Finished 112 episodes (112.0 cycles). mean_score_over_agents 28.54749936191365 trailing 15.61676965093799\n",
      "Finished 113 episodes (113.0 cycles). mean_score_over_agents 28.52549936240539 trailing 15.881444645022041\n",
      "Finished 114 episodes (114.0 cycles). mean_score_over_agents 28.18649936998263 trailing 16.142814639179967\n",
      "Finished 115 episodes (115.0 cycles). mean_score_over_agents 27.24549939101562 trailing 16.392804633592256\n",
      "Finished 116 episodes (116.0 cycles). mean_score_over_agents 27.21899939160794 trailing 16.64464962796308\n",
      "Finished 117 episodes (117.0 cycles). mean_score_over_agents 29.155999348312616 trailing 16.910319622024893\n",
      "Finished 118 episodes (118.0 cycles). mean_score_over_agents 25.483999430388213 trailing 17.137409616949036\n",
      "Finished 119 episodes (119.0 cycles). mean_score_over_agents 26.827999400347473 trailing 17.378524611559698\n",
      "Finished 120 episodes (120.0 cycles). mean_score_over_agents 25.518999429605902 trailing 17.60763460643869\n",
      "Finished 121 episodes (121.0 cycles). mean_score_over_agents 24.77449944624677 trailing 17.833094601399267\n",
      "Finished 122 episodes (122.0 cycles). mean_score_over_agents 25.53099942933768 trailing 18.05498459643964\n",
      "Finished 123 episodes (123.0 cycles). mean_score_over_agents 27.391999387741087 trailing 18.301834590922113\n",
      "Finished 124 episodes (124.0 cycles). mean_score_over_agents 27.50249938527122 trailing 18.54502958548628\n",
      "Finished 125 episodes (125.0 cycles). mean_score_over_agents 26.277499412652105 trailing 18.778314580271953\n",
      "Finished 126 episodes (126.0 cycles). mean_score_over_agents 27.748499379772692 trailing 19.02784957469441\n",
      "Finished 127 episodes (127.0 cycles). mean_score_over_agents 25.78399942368269 trailing 19.25105456970539\n",
      "Finished 128 episodes (128.0 cycles). mean_score_over_agents 26.958999397419394 trailing 19.48462956448458\n",
      "Finished 129 episodes (129.0 cycles). mean_score_over_agents 27.83649937780574 trailing 19.7269945590673\n",
      "Finished 130 episodes (130.0 cycles). mean_score_over_agents 29.892999331839384 trailing 19.982979553345587\n",
      "Finished 131 episodes (131.0 cycles). mean_score_over_agents 26.01399941854179 trailing 20.202289548443627\n",
      "Finished 132 episodes (132.0 cycles). mean_score_over_agents 26.398499409947544 trailing 20.419654543585143\n",
      "Finished 133 episodes (133.0 cycles). mean_score_over_agents 28.842499355319887 trailing 20.660664538198148\n",
      "Finished 134 episodes (134.0 cycles). mean_score_over_agents 29.41799934245646 trailing 20.909044532646426\n",
      "Finished 135 episodes (135.0 cycles). mean_score_over_agents 25.76599942408502 trailing 21.11821452797111\n",
      "Finished 136 episodes (136.0 cycles). mean_score_over_agents 30.824499311018734 trailing 21.37689952218905\n",
      "Finished 137 episodes (137.0 cycles). mean_score_over_agents 29.54149933969602 trailing 21.61043451696914\n",
      "Finished 138 episodes (138.0 cycles). mean_score_over_agents 28.947499352972955 trailing 21.839629511846233\n",
      "Finished 139 episodes (139.0 cycles). mean_score_over_agents 29.00999935157597 trailing 22.069739506702877\n",
      "Finished 140 episodes (140.0 cycles). mean_score_over_agents 30.193499325122684 trailing 22.29837950159237\n",
      "Finished 141 episodes (141.0 cycles). mean_score_over_agents 29.911499331425876 trailing 22.530284496408886\n",
      "Finished 142 episodes (142.0 cycles). mean_score_over_agents 27.846999377571045 trailing 22.738769491748887\n",
      "Finished 143 episodes (143.0 cycles). mean_score_over_agents 28.01349937384948 trailing 22.939899487253278\n",
      "Finished 144 episodes (144.0 cycles). mean_score_over_agents 30.441999319568275 trailing 23.16675448218268\n",
      "Finished 145 episodes (145.0 cycles). mean_score_over_agents 30.983499307464808 trailing 23.4077994767949\n",
      "Finished 146 episodes (146.0 cycles). mean_score_over_agents 30.194999325089157 trailing 23.621609472015866\n",
      "Finished 147 episodes (147.0 cycles). mean_score_over_agents 30.331999322026967 trailing 23.82926946737431\n",
      "Finished 148 episodes (148.0 cycles). mean_score_over_agents 26.841499400045723 trailing 23.987159463845195\n",
      "Finished 149 episodes (149.0 cycles). mean_score_over_agents 29.922499331180006 trailing 24.18809945935384\n",
      "Finished 150 episodes (150.0 cycles). mean_score_over_agents 29.29049934530631 trailing 24.36216445546318\n",
      "Finished 151 episodes (151.0 cycles). mean_score_over_agents 30.00999932922423 trailing 24.558709451070058\n",
      "Finished 152 episodes (152.0 cycles). mean_score_over_agents 29.758999334834517 trailing 24.747304446854628\n",
      "Finished 153 episodes (153.0 cycles). mean_score_over_agents 29.658499337080865 trailing 24.908224443257787\n",
      "Finished 154 episodes (154.0 cycles). mean_score_over_agents 30.474499318841843 trailing 25.074324439545165\n",
      "Finished 155 episodes (155.0 cycles). mean_score_over_agents 29.118499349150806 trailing 25.213584436432463\n",
      "Finished 156 episodes (156.0 cycles). mean_score_over_agents 30.269999323412776 trailing 25.370784432918764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 157 episodes (157.0 cycles). mean_score_over_agents 29.918999331258238 trailing 25.505574429905977\n",
      "Finished 158 episodes (158.0 cycles). mean_score_over_agents 29.39849934289232 trailing 25.63965442690905\n",
      "Finished 159 episodes (159.0 cycles). mean_score_over_agents 30.31649932237342 trailing 25.805914423192853\n",
      "Finished 160 episodes (160.0 cycles). mean_score_over_agents 30.33349932199344 trailing 25.961024419725874\n",
      "Finished 161 episodes (161.0 cycles). mean_score_over_agents 29.076999350078403 trailing 26.10129941659048\n",
      "Finished 162 episodes (162.0 cycles). mean_score_over_agents 27.893999376520515 trailing 26.223444413860328\n",
      "Finished 163 episodes (163.0 cycles). mean_score_over_agents 28.109499371703713 trailing 26.30014941214584\n",
      "Finished 164 episodes (164.0 cycles). mean_score_over_agents 28.465499363746495 trailing 26.416184409552258\n",
      "Finished 165 episodes (165.0 cycles). mean_score_over_agents 31.19099930282682 trailing 26.526869407078248\n",
      "Finished 166 episodes (166.0 cycles). mean_score_over_agents 30.781999311968683 trailing 26.661069404078646\n",
      "Finished 167 episodes (167.0 cycles). mean_score_over_agents 26.982499396894127 trailing 26.735544402413996\n",
      "Finished 168 episodes (168.0 cycles). mean_score_over_agents 29.115499349217863 trailing 26.837964400124733\n",
      "Finished 169 episodes (169.0 cycles). mean_score_over_agents 30.57549931658432 trailing 26.938179397884756\n",
      "Finished 170 episodes (170.0 cycles). mean_score_over_agents 28.324999366886914 trailing 27.012984396212733\n",
      "Finished 171 episodes (171.0 cycles). mean_score_over_agents 29.06699935030192 trailing 27.098374394304116\n",
      "Finished 172 episodes (172.0 cycles). mean_score_over_agents 29.270499345753343 trailing 27.176754392552187\n",
      "Finished 173 episodes (173.0 cycles). mean_score_over_agents 28.90599935390055 trailing 27.255564390790646\n",
      "Finished 174 episodes (174.0 cycles). mean_score_over_agents 28.394499365333466 trailing 27.303914389709938\n",
      "Finished 175 episodes (175.0 cycles). mean_score_over_agents 28.875999354571103 trailing 27.386294387868602\n",
      "Finished 176 episodes (176.0 cycles). mean_score_over_agents 29.482499341014773 trailing 27.442944386602377\n",
      "Finished 177 episodes (177.0 cycles). mean_score_over_agents 29.091999349743126 trailing 27.54204938438721\n",
      "Finished 178 episodes (178.0 cycles). mean_score_over_agents 27.292999389953913 trailing 27.569104383782477\n",
      "Finished 179 episodes (179.0 cycles). mean_score_over_agents 25.29599943459034 trailing 27.596024383180776\n",
      "Finished 180 episodes (180.0 cycles). mean_score_over_agents 27.156999392993747 trailing 27.63672438227106\n",
      "Finished 181 episodes (181.0 cycles). mean_score_over_agents 30.141999326273798 trailing 27.72338938033394\n",
      "Finished 182 episodes (182.0 cycles). mean_score_over_agents 28.143499370943754 trailing 27.784524378967472\n",
      "Finished 183 episodes (183.0 cycles). mean_score_over_agents 28.977999352291228 trailing 27.846294377586805\n",
      "Finished 184 episodes (184.0 cycles). mean_score_over_agents 29.393499343004077 trailing 27.90202437634114\n",
      "Finished 185 episodes (185.0 cycles). mean_score_over_agents 27.59099938329309 trailing 27.924354375842018\n",
      "Finished 186 episodes (186.0 cycles). mean_score_over_agents 30.655999314785003 trailing 27.997919374197714\n",
      "Finished 187 episodes (187.0 cycles). mean_score_over_agents 28.98299935217947 trailing 28.062104372763066\n",
      "Finished 188 episodes (188.0 cycles). mean_score_over_agents 29.6114993381314 trailing 28.12172437143046\n",
      "Finished 189 episodes (189.0 cycles). mean_score_over_agents 30.73399931304157 trailing 28.17495937024057\n",
      "Finished 190 episodes (190.0 cycles). mean_score_over_agents 28.36799936592579 trailing 28.215314369338568\n",
      "Finished 191 episodes (191.0 cycles). mean_score_over_agents 26.530999406985938 trailing 28.215249369340018\n",
      "Finished 192 episodes (192.0 cycles). mean_score_over_agents 22.46499949786812 trailing 28.1714593703188\n",
      "Finished 193 episodes (193.0 cycles). mean_score_over_agents 28.05299937296659 trailing 28.176489370206365\n",
      "Finished 194 episodes (194.0 cycles). mean_score_over_agents 28.964499352592973 trailing 28.20617436954286\n",
      "Finished 195 episodes (195.0 cycles). mean_score_over_agents 31.264499301183967 trailing 28.275454367994328\n",
      "Finished 196 episodes (196.0 cycles). mean_score_over_agents 29.083999349921942 trailing 28.286199367754165\n",
      "Finished 197 episodes (197.0 cycles). mean_score_over_agents 27.6014993830584 trailing 28.308694367251363\n",
      "Finished 198 episodes (198.0 cycles). mean_score_over_agents 29.804999333806336 trailing 28.348949366351594\n",
      "Finished 199 episodes (199.0 cycles). mean_score_over_agents 30.464999319054186 trailing 28.380304365650755\n",
      "Finished 200 episodes (200.0 cycles). mean_score_over_agents 30.561999316886066 trailing 28.434844364431694\n",
      "Finished 201 episodes (201.0 cycles). mean_score_over_agents 30.176499325502665 trailing 28.475274363528012\n",
      "Finished 202 episodes (202.0 cycles). mean_score_over_agents 27.624499382544307 trailing 28.47622436350678\n",
      "Finished 203 episodes (203.0 cycles). mean_score_over_agents 30.23199932426214 trailing 28.490374363190497\n",
      "Finished 204 episodes (204.0 cycles). mean_score_over_agents 30.49949931828305 trailing 28.542064362035138\n",
      "Finished 205 episodes (205.0 cycles). mean_score_over_agents 29.394499342981725 trailing 28.565264361516576\n",
      "Finished 206 episodes (206.0 cycles). mean_score_over_agents 31.24399930164218 trailing 28.602594360682186\n",
      "Finished 207 episodes (207.0 cycles). mean_score_over_agents 31.343999299407006 trailing 28.626319360151893\n",
      "Finished 208 episodes (208.0 cycles). mean_score_over_agents 29.84199933297932 trailing 28.663004359331918\n",
      "Finished 209 episodes (209.0 cycles). mean_score_over_agents 31.494999296031892 trailing 28.71682935812883\n",
      "Finished 210 episodes (210.0 cycles). mean_score_over_agents 32.63799927048385 trailing 28.77260935688205\n",
      "Finished 211 episodes (211.0 cycles). mean_score_over_agents 31.476999296434222 trailing 28.811174356020057\n",
      "Finished 212 episodes (212.0 cycles). mean_score_over_agents 31.63999929279089 trailing 28.842099355328823\n",
      "Finished 213 episodes (213.0 cycles). mean_score_over_agents 31.55199929475784 trailing 28.87236435465235\n",
      "Finished 214 episodes (214.0 cycles). mean_score_over_agents 32.44599927477539 trailing 28.91495935370028\n",
      "Finished 215 episodes (215.0 cycles). mean_score_over_agents 31.93549928618595 trailing 28.96185935265198\n",
      "Finished 216 episodes (216.0 cycles). mean_score_over_agents 31.92549928640947 trailing 29.008924351599994\n",
      "Finished 217 episodes (217.0 cycles). mean_score_over_agents 30.334499321971087 trailing 29.020709351336578\n",
      "Finished 218 episodes (218.0 cycles). mean_score_over_agents 32.1519992813468 trailing 29.087389349846166\n",
      "Finished 219 episodes (219.0 cycles). mean_score_over_agents 30.487999318540098 trailing 29.123989349028093\n",
      "Finished 220 episodes (220.0 cycles). mean_score_over_agents 29.98849932970479 trailing 29.168684348029082\n",
      "Finished 221 episodes (221.0 cycles). mean_score_over_agents 33.45449925223365 trailing 29.25548434608895\n",
      "Finished 222 episodes (222.0 cycles). mean_score_over_agents 33.47149925185367 trailing 29.33488934431411\n",
      "Finished 223 episodes (223.0 cycles). mean_score_over_agents 31.73099929075688 trailing 29.37827934334427\n",
      "Finished 224 episodes (224.0 cycles). mean_score_over_agents 30.71249931352213 trailing 29.41037934262678\n",
      "Finished 225 episodes (225.0 cycles). mean_score_over_agents 32.46199927441776 trailing 29.47222434124443\n",
      "Finished 226 episodes (226.0 cycles). mean_score_over_agents 30.272999323345722 trailing 29.497469340680162\n",
      "Finished 227 episodes (227.0 cycles). mean_score_over_agents 30.033499328698962 trailing 29.53996433973032\n",
      "Finished 228 episodes (228.0 cycles). mean_score_over_agents 30.67499931436032 trailing 29.577124338899736\n",
      "Finished 229 episodes (229.0 cycles). mean_score_over_agents 30.029999328777194 trailing 29.599059338409447\n",
      "Finished 230 episodes (230.0 cycles). mean_score_over_agents 33.60199924893677 trailing 29.636149337580424\n",
      "Finished 231 episodes (231.0 cycles). mean_score_over_agents 32.049999283626676 trailing 29.696509336231273\n",
      "Finished 232 episodes (232.0 cycles). mean_score_over_agents 29.310999344848096 trailing 29.725634335580274\n",
      "Finished 233 episodes (233.0 cycles). mean_score_over_agents 30.550499317143114 trailing 29.74271433519851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 234 episodes (234.0 cycles). mean_score_over_agents 31.968499285448342 trailing 29.768219334628427\n",
      "Finished 235 episodes (235.0 cycles). mean_score_over_agents 32.625999270752075 trailing 29.836819333095097\n",
      "Finished 236 episodes (236.0 cycles). mean_score_over_agents 31.30949930017814 trailing 29.841669332986694\n",
      "Finished 237 episodes (237.0 cycles). mean_score_over_agents 31.416499297786505 trailing 29.8604193325676\n",
      "Finished 238 episodes (238.0 cycles). mean_score_over_agents 30.57349931662902 trailing 29.876679332204162\n",
      "Finished 239 episodes (239.0 cycles). mean_score_over_agents 33.816999244131146 trailing 29.924749331129714\n",
      "Finished 240 episodes (240.0 cycles). mean_score_over_agents 28.84999935515225 trailing 29.911314331430006\n",
      "Finished 241 episodes (241.0 cycles). mean_score_over_agents 32.698499269131574 trailing 29.939184330807063\n",
      "Finished 242 episodes (242.0 cycles). mean_score_over_agents 34.079499238263814 trailing 30.00150932941399\n",
      "Breaking\n",
      "Reached target! mean_last_100 30.00150932941399\n",
      "======== Saving weights ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = MasterAgent(num_agents, state_size=state_size, action_size=action_size, seed=0)\n",
    "session = TrainingSession(num_agents)\n",
    "scores = session.train_ppo(agent, 30.0)   # Do the training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Policy\n",
    "The code below runs the policy that has previously been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run_ppo(agent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
