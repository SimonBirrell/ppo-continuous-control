{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher20.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "ROLLOUT_LENGTH = 250 \n",
    "DISCOUNT = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "OPTIMIZATION_EPOCHS = 10\n",
    "MINI_BATCH_SIZE = 64\n",
    "PPO_RATIO_CLIP = 0.1\n",
    "GRADIENT_CLIP = 0.75\n",
    "HIDDEN_LAYERS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-party Code\n",
    "These are helper routines copied or adapted from other projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to Shangtong Zhang \n",
    "# https://github.com/ShangtongZhang/DeepRL\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "def to_np(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "def tensor(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = torch.tensor(x, device=DEVICE, dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]\n",
    "\n",
    "class MeanStdNormalizer():\n",
    "    def __init__(self):\n",
    "        self.rms = None\n",
    "        self.clip = 10.0\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.rms is None:\n",
    "            self.rms = RunningMeanStd(shape=(1, ) + x.shape[1:])\n",
    "        self.rms.update(x)\n",
    "        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n",
    "                       -self.clip, self.clip)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to OpenAI.\n",
    "\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/running_mean_std.py    \n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)    \n",
    "        \n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Agent\n",
    "\n",
    "The master agent implements the PPO algorithm and can use multiple sub-agents for the purpose of samlping trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class SubNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, output_size, seed):\n",
    "        super(SubNetwork, self).__init__()\n",
    "        dims = (input_size,) + hidden_units        \n",
    "        self.layers = nn.ModuleList([layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.feature_dim = dims[-1]\n",
    "        self.output_layer = layer_init(nn.Linear(self.feature_dim, output_size), 1e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.tanh(layer(x))\n",
    "        x = self.output_layer(x)    \n",
    "        return x    \n",
    "            \n",
    "class ActorAndCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        super(ActorAndCritic, self).__init__()\n",
    "        self.seed = random.seed(seed)\n",
    "        self.actor = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), action_size, seed)\n",
    "        self.critic = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), 1, seed)\n",
    "        self.std = nn.Parameter(torch.zeros(action_size))\n",
    "        #self.to(Config.DEVICE)\n",
    "        \n",
    "    def forward(self, obs, action=None):\n",
    "        obs = tensor(obs)\n",
    "        a = self.actor(obs)\n",
    "        v = self.critic(obs)\n",
    "        mean = F.tanh(a)\n",
    "        dist = torch.distributions.Normal(mean, F.softplus(self.std))\n",
    "        return (v, dist)\n",
    "        \n",
    "class Rollout():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stored values\n",
    "        self.actions = []\n",
    "        self.log_prob_actions = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.episode_not_dones = []\n",
    "        self.states = []\n",
    "        # Calculated values\n",
    "        self.returns = [0.0] * ROLLOUT_LENGTH\n",
    "        self.advantages = [0.0] * ROLLOUT_LENGTH\n",
    "        \n",
    "    def save_prediction(self, actions, log_prob_actions, values):\n",
    "        self.actions.append(actions)\n",
    "        self.log_prob_actions.append(log_prob_actions)\n",
    "        self.values.append(values)\n",
    "\n",
    "    def save_consequences(self, rewards, episode_not_dones, states):\n",
    "        self.rewards.append(rewards)\n",
    "        self.episode_not_dones.append(episode_not_dones)\n",
    "        self.states.append(states)\n",
    "        \n",
    "    def calculate_returns_and_advantages(self, final_reward):\n",
    "        self.rewards.append(None)\n",
    "        self.episode_not_dones.append(None)\n",
    "        self.calculate_future_returns(final_reward)\n",
    "        self.estimate_advantages()\n",
    "\n",
    "    def calculate_future_returns(self, returns):\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            returns = self.rewards[i] + DISCOUNT * self.episode_not_dones[i] * returns\n",
    "            self.returns[i] = returns.detach() \n",
    "\n",
    "    def estimate_advantages(self):\n",
    "        advantages = tensor(np.zeros((num_agents, 1)))\n",
    "        # Go backwards through rollout steps and calculate advantages for each state action pair\n",
    "        # Use GAE for PPO. (Schulman, Moritz, Levine et al. 2016)\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            td = self.rewards[i] + (DISCOUNT * self.episode_not_dones[i] * self.values[i + 1]) - self.values[i]\n",
    "            advantages = advantages * GAE_LAMBDA * DISCOUNT * self.episode_not_dones[i] + td\n",
    "            self.advantages[i] = advantages.detach()               \n",
    "\n",
    "    def stack_tensor(self, some_list):\n",
    "        return torch.cat(some_list[:ROLLOUT_LENGTH], dim=0)\n",
    "            \n",
    "    def get_sample_data(self):\n",
    "        states = self.stack_tensor(self.states)\n",
    "        actions = self.stack_tensor(self.actions) \n",
    "        log_prob_actions = self.stack_tensor(self.log_prob_actions)\n",
    "        returns = self.stack_tensor(self.returns)\n",
    "        # Normalize advantages\n",
    "        advantages = self.stack_tensor(self.advantages)\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()        \n",
    "        return (states, actions, log_prob_actions, returns, advantages)\n",
    "    \n",
    "class MasterAgent():   \n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        self.network = ActorAndCritic(num_agents, state_size, action_size, seed)\n",
    "        self.first_states = True\n",
    "        self.total_steps = 0\n",
    "        self.state_normalizer = MeanStdNormalizer()\n",
    "        \n",
    "    def evaluate_actions_against_states(self, states, actions):\n",
    "        value, action_distribution = self.network(states, actions)\n",
    "        log_prob = self.get_log_prob(action_distribution, actions)\n",
    "        return (log_prob, value)\n",
    "    \n",
    "    def get_log_prob(self, action_distribution, actions):\n",
    "        return action_distribution.log_prob(actions).sum(-1).unsqueeze(-1)\n",
    "    \n",
    "    def get_prediction(self, states):\n",
    "        if self.first_states:\n",
    "            self.states = states\n",
    "            self.first_states = False\n",
    "        #self.latest_actions, self.latest_log_prob, self.latest_values = self.get_prediction_from_states(self.states)\n",
    "        self.latest_values, action_distribution = self.network(self.states)\n",
    "        self.latest_actions = action_distribution.sample()\n",
    "        self.latest_log_prob = self.get_log_prob(action_distribution, self.latest_actions)\n",
    "        return self.latest_actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        rewards = np.asarray(rewards)\n",
    "        next_states = self.state_normalizer(next_states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "        dones = np.asarray(dones).astype(int)\n",
    "        rewards = tensor(rewards).unsqueeze(-1)\n",
    "        episode_not_dones = tensor(1 - dones).unsqueeze(-1)\n",
    "        states = tensor(self.states)        \n",
    "        self.rollout.save_consequences(rewards, episode_not_dones, states)\n",
    "\n",
    "        self.states = next_states\n",
    "                \n",
    "    def start_rollout(self):\n",
    "        self.rollout = Rollout()\n",
    "            \n",
    "    def process_rollout(self, states):\n",
    "        self.save_final_results(states)\n",
    "        self.rollout.calculate_returns_and_advantages(self.latest_values.detach())\n",
    "        self.optimize()\n",
    "        self.first_states = True\n",
    "        \n",
    "    def save_final_results(self, states):    \n",
    "        self.get_prediction(states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "   \n",
    "    def optimize(self):\n",
    "        # Now use tensors for 's', 'a', 'log_pi_a', 'ret', 'adv' for training\n",
    "        # states, actions, log prob actions, returns, advantages (1 row / timestep, 1 column per worker)\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = self.rollout.get_sample_data()\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        optimizer = torch.optim.Adam(self.network.parameters(), 3e-4, eps=1e-5)\n",
    "        for i in range(OPTIMIZATION_EPOCHS):\n",
    "            number_timesteps = states.size(0)\n",
    "            timesteps_to_sample = random_sample(np.arange(number_timesteps), MINI_BATCH_SIZE) \n",
    "            for timestep in timesteps_to_sample:\n",
    "                t = tensor(timestep).long()\n",
    "                # Get data for all workers from sampled timestep \n",
    "                sampled_states = states[t]\n",
    "                sampled_actions = actions[t]\n",
    "                sampled_log_probs_old = log_probs_old[t]\n",
    "                sampled_returns = returns[t]\n",
    "                sampled_advantages = advantages[t]\n",
    "                self.optimize_with_sampled_worker_data(optimizer, sampled_states,\n",
    "                                                                  sampled_actions,\n",
    "                                                                  sampled_log_probs_old,\n",
    "                                                                  sampled_returns,\n",
    "                                                                  sampled_advantages)\n",
    "        steps = ROLLOUT_LENGTH * num_agents\n",
    "        # Total steps used to train network\n",
    "        self.total_steps += steps\n",
    "        \n",
    "    def optimize_with_sampled_worker_data(self, optimizer, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages):\n",
    "        # Get log_prob(actions) and value given states\n",
    "        # Pass in states for all workers x batch_size.\n",
    "        log_prob_action, value = self.evaluate_actions_against_states(sampled_states, sampled_actions)\n",
    "        \n",
    "        policy_loss = self.get_policy_loss(log_prob_action, sampled_log_probs_old, sampled_advantages)\n",
    "        value_loss = self.get_value_loss(value, sampled_returns)\n",
    "        \n",
    "        # Do the actual optimization\n",
    "        optimizer.zero_grad()\n",
    "        # Overall loss function for training both networks at once. Get gradients on weights.\n",
    "        (policy_loss + value_loss).backward()\n",
    "        # Clip weight gradients \n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP) \n",
    "        # Run actual optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    def get_policy_loss(self, log_prob_action, sampled_log_probs_old, sampled_advantages):\n",
    "        # This is the core of PPO\n",
    "        # ratio = new prob / old prob for all workers\n",
    "        ratio = (log_prob_action - sampled_log_probs_old).exp() \n",
    "        # Clip loss on the upside\n",
    "        clamped_ratio = ratio.clamp(1.0 - PPO_RATIO_CLIP, 1.0 + PPO_RATIO_CLIP)\n",
    "        obj = ratio * sampled_advantages\n",
    "        obj_clipped = clamped_ratio * sampled_advantages\n",
    "        policy_loss = -torch.min(obj, obj_clipped).mean() \n",
    "        return policy_loss\n",
    "    \n",
    "    def get_value_loss(self, value, sampled_returns):\n",
    "        # Mean squared error\n",
    "        value_loss = 0.5 * (sampled_returns - value).pow(2).mean()\n",
    "        return value_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The TrainingSession class trains the agent while monitoring the progress of the episodes. It can also simply run an episode with the previously trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSession():\n",
    "    \n",
    "    def __init__(self, num_workers):\n",
    "        self.num_workers = num_workers\n",
    "        self.online_rewards = np.zeros(num_workers)\n",
    "        self.mean_last_100 = 0\n",
    "        self.mean_scores = []\n",
    "\n",
    "    def train_ppo(self, agent, target_average_score, max_episodes=300):\n",
    "        print(\"Attempting to reach 100 episode trailing average of {:.2f} in under {} episodes.\".format(target_average_score, max_episodes))\n",
    "        print(\"Rollout length: %s\" % ROLLOUT_LENGTH)\n",
    "        print(\"GRADIENT_CLIP %s\" % GRADIENT_CLIP)\n",
    "        print(\"PPO_RATIO_CLIP %s\" % PPO_RATIO_CLIP)\n",
    "        print(\"GAE_LAMBDA %s\" % GAE_LAMBDA)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment   \n",
    "        self.num_episodes = 0\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.episode_scores = [list() for i in range(num_agents)]\n",
    "        self.episodes_finished = 0\n",
    "        self.target_average_score = target_average_score\n",
    "        while True:\n",
    "            mean_last_100 = self.run_rollout(agent, env_info)\n",
    "            agent.process_rollout(states)\n",
    "            \n",
    "            #print(\"\\rEpisode {}\\tLast 100: {:.2f}\".format(self.num_episodes, mean_last_100))\n",
    "            if mean_last_100 > target_average_score:\n",
    "                print(\"Reached target! mean_last_100 %s\" % mean_last_100)\n",
    "                break\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Failed to reach target in {} episodes.\".format(self.num_episodes))\n",
    "                break\n",
    "        return self.mean_scores \n",
    "    \n",
    "    def get_actions_from_policy(self, states):\n",
    "        actions = agent.get_prediction(states)          # Run the policy \n",
    "        actions = to_np(actions)                        # Extract actions\n",
    "        actions = np.clip(actions, -1, 1)               # all actions between -1 and 1\n",
    "        return actions\n",
    "    \n",
    "    def run_ppo(self, agent, max_episodes=5):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.num_episodes = 0\n",
    "        while True:\n",
    "            mean_score_over_agents = self.run_rollout(agent, env_info)\n",
    "            self.num_episodes += 1\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Policy failed to reach target in %s\" % max_episodes)\n",
    "                break\n",
    "    \n",
    "    def step_environment(self, env, actions):\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment        \n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished for each agent\n",
    "        return (next_states, rewards, dones)\n",
    "                                \n",
    "    def run_episode(self, env_info):\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        while True:\n",
    "            actions = self.get_actions_from_policy(states)\n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "            \n",
    "    def run_rollout(self, agent, env_info):\n",
    "        #print(\"Run rollout\")\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        for t in range(ROLLOUT_LENGTH):\n",
    "            actions = self.get_actions_from_policy(states)            \n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "        \n",
    "            # Save rewards\n",
    "            self.online_rewards += rewards                          # Accumulate ongoing (un-normalized) rewards for each agent\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:                                       # For a worker whose episode is done...\n",
    "                    #print(\"Worker %s finished at timestep %s\" % (i, t))\n",
    "                    self.end_episode(i, self.online_rewards[i])\n",
    "                    self.online_rewards[i] = 0                 # Reset accumulated reward for next episode\n",
    "                    self.mean_last_100 = np.mean(self.last_100_scores)\n",
    "                    if self.mean_last_100 > self.target_average_score:\n",
    "                        print(\"Breaking\")\n",
    "                        break\n",
    "            #print(\"%s step\" % t)            \n",
    "            agent.step(states, actions, rewards, next_states, dones) # Teach the agent \n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "\n",
    "        #print(\"Steps in rollout: {}\".format(t+1))        \n",
    "        return self.mean_last_100    \n",
    "\n",
    "    def end_episode(self, agent_index, score):\n",
    "        self.episode_scores[agent_index].append(score)   # Save the reward they accumulated in the episode\n",
    "        self.episodes_finished +=1\n",
    "        if (self.episodes_finished % num_agents) == 0:\n",
    "            self.num_episodes += 1\n",
    "            total_over_agents = 0\n",
    "            for i in range(num_agents):\n",
    "                total_over_agents += self.episode_scores[i][-1]\n",
    "            mean_score_over_agents = total_over_agents / num_agents    \n",
    "            self.last_100_scores.append(mean_score_over_agents)\n",
    "            self.mean_scores.append(mean_score_over_agents)\n",
    "            print(\"Finished %s episodes (%s cycles). mean_score_over_agents %s trailing %s\" % (self.num_episodes, (self.episodes_finished/num_agents), mean_score_over_agents, np.mean(self.last_100_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Policy\n",
    "\n",
    "Running the code in the cell below trains the policy, attempting to reach the target treailing avewrage value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reach 100 episode trailing average of 30.00 in under 300 episodes.\n",
      "Rollout length: 250\n",
      "GRADIENT_CLIP 0.75\n",
      "PPO_RATIO_CLIP 0.1\n",
      "GAE_LAMBDA 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/drlnd/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda2/envs/drlnd/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 episodes (1.0 cycles). mean_score_over_agents 0.05199999883770943 trailing 0.05199999883770943\n",
      "Finished 2 episodes (2.0 cycles). mean_score_over_agents 0.11199999749660491 trailing 0.08199999816715717\n",
      "Finished 3 episodes (3.0 cycles). mean_score_over_agents 0.1424999968148768 trailing 0.10216666438306371\n",
      "Finished 4 episodes (4.0 cycles). mean_score_over_agents 0.23149999482557176 trailing 0.13449999699369072\n",
      "Finished 5 episodes (5.0 cycles). mean_score_over_agents 0.4494999899528921 trailing 0.197499995585531\n",
      "Finished 6 episodes (6.0 cycles). mean_score_over_agents 0.5524999876506627 trailing 0.2566666609297196\n",
      "Finished 7 episodes (7.0 cycles). mean_score_over_agents 0.5879999868571758 trailing 0.3039999932050705\n",
      "Finished 8 episodes (8.0 cycles). mean_score_over_agents 0.7944999822415412 trailing 0.3653124918346293\n",
      "Finished 9 episodes (9.0 cycles). mean_score_over_agents 0.8389999812468887 trailing 0.4179444351026581\n",
      "Finished 10 episodes (10.0 cycles). mean_score_over_agents 1.1394999745301901 trailing 0.4900999890454113\n",
      "Finished 11 episodes (11.0 cycles). mean_score_over_agents 1.487499966751784 trailing 0.5807727142914452\n",
      "Finished 12 episodes (12.0 cycles). mean_score_over_agents 1.5809999646618962 trailing 0.6641249851556494\n",
      "Finished 13 episodes (13.0 cycles). mean_score_over_agents 1.5794999646954238 trailing 0.7345384451202475\n",
      "Finished 14 episodes (14.0 cycles). mean_score_over_agents 1.9004999575205148 trailing 0.8178214102916951\n",
      "Finished 15 episodes (15.0 cycles). mean_score_over_agents 1.8814999579451979 trailing 0.8887333134685953\n",
      "Finished 16 episodes (16.0 cycles). mean_score_over_agents 1.8699999582022429 trailing 0.9500624787644483\n",
      "Finished 17 episodes (17.0 cycles). mean_score_over_agents 1.649999963119626 trailing 0.9912352719618117\n",
      "Finished 18 episodes (18.0 cycles). mean_score_over_agents 2.222499950323254 trailing 1.059638865204114\n",
      "Finished 19 episodes (19.0 cycles). mean_score_over_agents 2.172999951429665 trailing 1.118236817110722\n",
      "Finished 20 episodes (20.0 cycles). mean_score_over_agents 2.072499953676015 trailing 1.1659499739389867\n",
      "Finished 21 episodes (21.0 cycles). mean_score_over_agents 1.948499956447631 trailing 1.2032142588203507\n",
      "Finished 22 episodes (22.0 cycles). mean_score_over_agents 2.621999941393733 trailing 1.2677045171191408\n",
      "Finished 23 episodes (23.0 cycles). mean_score_over_agents 2.4469999453052877 trailing 1.3189782313881036\n",
      "Finished 24 episodes (24.0 cycles). mean_score_over_agents 2.56349994270131 trailing 1.3708333026928206\n",
      "Finished 25 episodes (25.0 cycles). mean_score_over_agents 2.9314999344758688 trailing 1.4332599679641425\n",
      "Finished 26 episodes (26.0 cycles). mean_score_over_agents 2.679999940097332 trailing 1.4812115053538806\n",
      "Finished 27 episodes (27.0 cycles). mean_score_over_agents 3.345999925211072 trailing 1.5502777431263692\n",
      "Finished 28 episodes (28.0 cycles). mean_score_over_agents 3.910999912582338 trailing 1.6345892491783682\n",
      "Finished 29 episodes (29.0 cycles). mean_score_over_agents 3.5964999196119605 trailing 1.702241341262285\n",
      "Finished 30 episodes (30.0 cycles). mean_score_over_agents 3.30699992608279 trailing 1.755733294089635\n",
      "Finished 31 episodes (31.0 cycles). mean_score_over_agents 3.902999912761152 trailing 1.8249999592080712\n",
      "Finished 32 episodes (32.0 cycles). mean_score_over_agents 4.176999906636775 trailing 1.898499957565218\n",
      "Finished 33 episodes (33.0 cycles). mean_score_over_agents 4.04199990965426 trailing 1.9634545015679163\n",
      "Finished 34 episodes (34.0 cycles). mean_score_over_agents 3.9064999126829205 trailing 2.020602896012475\n",
      "Finished 35 episodes (35.0 cycles). mean_score_over_agents 3.9189999124035237 trailing 2.074842810766505\n",
      "Finished 36 episodes (36.0 cycles). mean_score_over_agents 4.61599989682436 trailing 2.1454305076014455\n",
      "Finished 37 episodes (37.0 cycles). mean_score_over_agents 4.673499895539135 trailing 2.2137567072754374\n",
      "Finished 38 episodes (38.0 cycles). mean_score_over_agents 4.533499898668379 trailing 2.2748025807331462\n",
      "Finished 39 episodes (39.0 cycles). mean_score_over_agents 4.518999898992479 trailing 2.3323461017141547\n",
      "Finished 40 episodes (40.0 cycles). mean_score_over_agents 5.053999887034297 trailing 2.400387446347158\n",
      "Finished 41 episodes (41.0 cycles). mean_score_over_agents 4.590499897394329 trailing 2.4538048232019674\n",
      "Finished 42 episodes (42.0 cycles). mean_score_over_agents 4.459499900322408 trailing 2.5015594678953113\n",
      "Finished 43 episodes (43.0 cycles). mean_score_over_agents 5.66699987333268 trailing 2.5751743610450175\n",
      "Finished 44 episodes (44.0 cycles). mean_score_over_agents 5.3719998799264435 trailing 2.638738577383232\n",
      "Finished 45 episodes (45.0 cycles). mean_score_over_agents 5.664499873388559 trailing 2.7059777172944615\n",
      "Finished 46 episodes (46.0 cycles). mean_score_over_agents 5.621999874338508 trailing 2.7693695033171584\n",
      "Finished 47 episodes (47.0 cycles). mean_score_over_agents 6.329499858524651 trailing 2.8451169576832753\n",
      "Finished 48 episodes (48.0 cycles). mean_score_over_agents 6.401499856915325 trailing 2.919208268083942\n",
      "Finished 49 episodes (49.0 cycles). mean_score_over_agents 5.797499870415777 trailing 2.97794891302949\n",
      "Finished 50 episodes (50.0 cycles). mean_score_over_agents 6.494499854836613 trailing 3.048279931865632\n",
      "Finished 51 episodes (51.0 cycles). mean_score_over_agents 7.537499831523746 trailing 3.1363038514667716\n",
      "Finished 52 episodes (52.0 cycles). mean_score_over_agents 6.887499846052378 trailing 3.2084422359780334\n",
      "Finished 53 episodes (53.0 cycles). mean_score_over_agents 6.931499845068902 trailing 3.27868860596088\n",
      "Finished 54 episodes (54.0 cycles). mean_score_over_agents 7.879499823879451 trailing 3.363888813700113\n",
      "Finished 55 episodes (55.0 cycles). mean_score_over_agents 6.709499850030989 trailing 3.4247181052697653\n",
      "Finished 56 episodes (56.0 cycles). mean_score_over_agents 8.084499819297344 trailing 3.5079284930202577\n",
      "Finished 57 episodes (57.0 cycles). mean_score_over_agents 8.437499811407179 trailing 3.594412200360379\n",
      "Finished 58 episodes (58.0 cycles). mean_score_over_agents 9.445999788865446 trailing 3.6953016415415005\n",
      "Finished 59 episodes (59.0 cycles). mean_score_over_agents 9.01149979857728 trailing 3.7854066950505816\n",
      "Finished 60 episodes (60.0 cycles). mean_score_over_agents 9.417999789491295 trailing 3.8792832466245937\n",
      "Finished 61 episodes (61.0 cycles). mean_score_over_agents 11.517999742552638 trailing 4.004508107213578\n",
      "Finished 62 episodes (62.0 cycles). mean_score_over_agents 11.490999743156134 trailing 4.125257972309425\n",
      "Finished 63 episodes (63.0 cycles). mean_score_over_agents 11.052999752946198 trailing 4.235222127557628\n",
      "Finished 64 episodes (64.0 cycles). mean_score_over_agents 13.174499705526978 trailing 4.374898339713399\n",
      "Finished 65 episodes (65.0 cycles). mean_score_over_agents 14.31549968002364 trailing 4.527830668025865\n",
      "Finished 66 episodes (66.0 cycles). mean_score_over_agents 15.246999659202993 trailing 4.690242319407336\n",
      "Finished 67 episodes (67.0 cycles). mean_score_over_agents 14.929499666299671 trailing 4.843067055928117\n",
      "Finished 68 episodes (68.0 cycles). mean_score_over_agents 15.05399966351688 trailing 4.993227829569128\n",
      "Finished 69 episodes (69.0 cycles). mean_score_over_agents 17.974499598238616 trailing 5.1813622030281055\n",
      "Finished 70 episodes (70.0 cycles). mean_score_over_agents 18.29299959111959 trailing 5.36867130857227\n",
      "Finished 71 episodes (71.0 cycles). mean_score_over_agents 16.7059996265918 trailing 5.528351989107756\n",
      "Finished 72 episodes (72.0 cycles). mean_score_over_agents 18.183999593555928 trailing 5.70412487250287\n",
      "Finished 73 episodes (73.0 cycles). mean_score_over_agents 18.655499583017082 trailing 5.881540964427723\n",
      "Finished 74 episodes (74.0 cycles). mean_score_over_agents 18.51249958621338 trailing 6.052229594451854\n",
      "Finished 75 episodes (75.0 cycles). mean_score_over_agents 22.05049950713292 trailing 6.265539859954268\n",
      "Finished 76 episodes (76.0 cycles). mean_score_over_agents 21.414499521348624 trailing 6.464868276551562\n",
      "Finished 77 episodes (77.0 cycles). mean_score_over_agents 22.4064994991757 trailing 6.671902448273952\n",
      "Finished 78 episodes (78.0 cycles). mean_score_over_agents 25.937999420240523 trailing 6.918903691504293\n",
      "Finished 79 episodes (79.0 cycles). mean_score_over_agents 24.254999457858503 trailing 7.1383479417113085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 80 episodes (80.0 cycles). mean_score_over_agents 26.37749941041693 trailing 7.378837335070129\n",
      "Finished 81 episodes (81.0 cycles). mean_score_over_agents 24.599499450158326 trailing 7.591438101923069\n",
      "Finished 82 episodes (82.0 cycles). mean_score_over_agents 28.821999355778097 trailing 7.850347385506668\n",
      "Finished 83 episodes (83.0 cycles). mean_score_over_agents 27.134999393485487 trailing 8.082692590422075\n",
      "Finished 84 episodes (84.0 cycles). mean_score_over_agents 28.300999367423355 trailing 8.323386718719709\n",
      "Finished 85 episodes (85.0 cycles). mean_score_over_agents 27.468999386020005 trailing 8.548629220687948\n",
      "Finished 86 episodes (86.0 cycles). mean_score_over_agents 29.336499344278128 trailing 8.790348640729693\n",
      "Finished 87 episodes (87.0 cycles). mean_score_over_agents 25.748499424476176 trailing 8.985269914106091\n",
      "Finished 88 episodes (88.0 cycles). mean_score_over_agents 29.396499342937023 trailing 9.21721570307008\n",
      "Finished 89 episodes (89.0 cycles). mean_score_over_agents 28.452999364025892 trailing 9.433348103754977\n",
      "Finished 90 episodes (90.0 cycles). mean_score_over_agents 27.808999378420413 trailing 9.637522006806813\n",
      "Finished 91 episodes (91.0 cycles). mean_score_over_agents 28.949999352917075 trailing 9.849747032588246\n",
      "Finished 92 episodes (92.0 cycles). mean_score_over_agents 30.015999329090118 trailing 10.06894542711544\n",
      "Finished 93 episodes (93.0 cycles). mean_score_over_agents 31.766999289952217 trailing 10.302257834242717\n",
      "Finished 94 episodes (94.0 cycles). mean_score_over_agents 31.0039993070066 trailing 10.522489126506162\n",
      "Finished 95 episodes (95.0 cycles). mean_score_over_agents 25.13849943811074 trailing 10.676341866628317\n",
      "Finished 96 episodes (96.0 cycles). mean_score_over_agents 34.5889992268756 trailing 10.925432047464225\n",
      "Finished 97 episodes (97.0 cycles). mean_score_over_agents 28.896999354101716 trailing 11.110705937223376\n",
      "Finished 98 episodes (98.0 cycles). mean_score_over_agents 32.21249927999452 trailing 11.326030359088387\n",
      "Finished 99 episodes (99.0 cycles). mean_score_over_agents 31.126999304257332 trailing 11.526040146413326\n",
      "Finished 100 episodes (100.0 cycles). mean_score_over_agents 32.778499267343435 trailing 11.738564737622628\n",
      "Finished 101 episodes (101.0 cycles). mean_score_over_agents 29.385499343182893 trailing 12.03189973106608\n",
      "Finished 102 episodes (102.0 cycles). mean_score_over_agents 30.269999323412776 trailing 12.33347972432524\n",
      "Finished 103 episodes (103.0 cycles). mean_score_over_agents 30.016999329067765 trailing 12.632224717647768\n",
      "Finished 104 episodes (104.0 cycles). mean_score_over_agents 31.012999306805433 trailing 12.940039710767566\n",
      "Finished 105 episodes (105.0 cycles). mean_score_over_agents 31.680499291885646 trailing 13.252349703786894\n",
      "Finished 106 episodes (106.0 cycles). mean_score_over_agents 31.562999294511975 trailing 13.562454696855507\n",
      "Finished 107 episodes (107.0 cycles). mean_score_over_agents 32.243499279301616 trailing 13.87900968977995\n",
      "Finished 108 episodes (108.0 cycles). mean_score_over_agents 34.624499226082115 trailing 14.217309682218358\n",
      "Finished 109 episodes (109.0 cycles). mean_score_over_agents 30.676999314315616 trailing 14.515689675549044\n",
      "Finished 110 episodes (110.0 cycles). mean_score_over_agents 32.37649927632883 trailing 14.828059668567034\n",
      "Finished 111 episodes (111.0 cycles). mean_score_over_agents 35.76849920051173 trailing 15.170869660904632\n",
      "Finished 112 episodes (112.0 cycles). mean_score_over_agents 32.392499275971204 trailing 15.478984654017724\n",
      "Finished 113 episodes (113.0 cycles). mean_score_over_agents 30.400499320495875 trailing 15.767194647575728\n",
      "Finished 114 episodes (114.0 cycles). mean_score_over_agents 33.31299925539643 trailing 16.081319640554486\n",
      "Finished 115 episodes (115.0 cycles). mean_score_over_agents 30.303499322663992 trailing 16.365539634201674\n",
      "Finished 116 episodes (116.0 cycles). mean_score_over_agents 30.406999320350586 trailing 16.65090962782316\n",
      "Finished 117 episodes (117.0 cycles). mean_score_over_agents 31.033999306336046 trailing 16.944749621255323\n",
      "Finished 118 episodes (118.0 cycles). mean_score_over_agents 29.862999332509936 trailing 17.22115461507719\n",
      "Finished 119 episodes (119.0 cycles). mean_score_over_agents 31.560499294567855 trailing 17.51502960850857\n",
      "Finished 120 episodes (120.0 cycles). mean_score_over_agents 34.448499230016026 trailing 17.83878960127197\n",
      "Finished 121 episodes (121.0 cycles). mean_score_over_agents 32.57599927186966 trailing 18.145064594426188\n",
      "Finished 122 episodes (122.0 cycles). mean_score_over_agents 27.798499378655105 trailing 18.396829588798806\n",
      "Finished 123 episodes (123.0 cycles). mean_score_over_agents 32.841999265924095 trailing 18.700779582004994\n",
      "Finished 124 episodes (124.0 cycles). mean_score_over_agents 31.73799929060042 trailing 18.992524575483984\n",
      "Finished 125 episodes (125.0 cycles). mean_score_over_agents 33.98299924042076 trailing 19.303039568543433\n",
      "Finished 126 episodes (126.0 cycles). mean_score_over_agents 33.88349924264476 trailing 19.615074561568907\n",
      "Finished 127 episodes (127.0 cycles). mean_score_over_agents 32.921999264135955 trailing 19.910834554958157\n",
      "Finished 128 episodes (128.0 cycles). mean_score_over_agents 33.81999924406409 trailing 20.209924548272976\n",
      "Finished 129 episodes (129.0 cycles). mean_score_over_agents 34.80899922195822 trailing 20.522049541296436\n",
      "Finished 130 episodes (130.0 cycles). mean_score_over_agents 32.43149927509948 trailing 20.81329453478661\n",
      "Finished 131 episodes (131.0 cycles). mean_score_over_agents 34.115499237459154 trailing 21.115419528033584\n",
      "Finished 132 episodes (132.0 cycles). mean_score_over_agents 34.05249923886731 trailing 21.41417452135589\n",
      "Finished 133 episodes (133.0 cycles). mean_score_over_agents 32.199999280273914 trailing 21.695754515062085\n",
      "Finished 134 episodes (134.0 cycles). mean_score_over_agents 34.42349923057482 trailing 22.000924508241006\n",
      "Finished 135 episodes (135.0 cycles). mean_score_over_agents 30.76699931230396 trailing 22.269404502240008\n",
      "Finished 136 episodes (136.0 cycles). mean_score_over_agents 30.85199931040406 trailing 22.531764496375807\n",
      "Finished 137 episodes (137.0 cycles). mean_score_over_agents 34.50249922880903 trailing 22.830054489708505\n",
      "Finished 138 episodes (138.0 cycles). mean_score_over_agents 32.955999263376 trailing 23.114279483355585\n",
      "Finished 139 episodes (139.0 cycles). mean_score_over_agents 33.71949924631044 trailing 23.40628447682876\n",
      "Finished 140 episodes (140.0 cycles). mean_score_over_agents 35.076499215979126 trailing 23.706509470118213\n",
      "Finished 141 episodes (141.0 cycles). mean_score_over_agents 37.18799916878343 trailing 24.032484462832098\n",
      "Finished 142 episodes (142.0 cycles). mean_score_over_agents 35.81249919952825 trailing 24.34601445582416\n",
      "Finished 143 episodes (143.0 cycles). mean_score_over_agents 34.647499225568026 trailing 24.635819449346513\n",
      "Finished 144 episodes (144.0 cycles). mean_score_over_agents 34.41949923066422 trailing 24.92629444285389\n",
      "Finished 145 episodes (145.0 cycles). mean_score_over_agents 34.36949923178181 trailing 25.213344436437822\n",
      "Finished 146 episodes (146.0 cycles). mean_score_over_agents 36.249499189760535 trailing 25.519619429592044\n",
      "Finished 147 episodes (147.0 cycles). mean_score_over_agents 35.50749920634553 trailing 25.811399423070256\n",
      "Finished 148 episodes (148.0 cycles). mean_score_over_agents 34.39399923123419 trailing 26.09132441681344\n",
      "Finished 149 episodes (149.0 cycles). mean_score_over_agents 34.28699923362583 trailing 26.376219410445547\n",
      "Finished 150 episodes (150.0 cycles). mean_score_over_agents 35.98749919561669 trailing 26.67114940385334\n",
      "Finished 151 episodes (151.0 cycles). mean_score_over_agents 34.97599921822548 trailing 26.945534397720362\n",
      "Finished 152 episodes (152.0 cycles). mean_score_over_agents 33.7289992460981 trailing 27.213949391720817\n",
      "Finished 153 episodes (153.0 cycles). mean_score_over_agents 33.84199924357235 trailing 27.48305438570585\n",
      "Finished 154 episodes (154.0 cycles). mean_score_over_agents 34.493999228999016 trailing 27.74919937975705\n",
      "Finished 155 episodes (155.0 cycles). mean_score_over_agents 34.16549923634157 trailing 28.023759373620155\n",
      "Finished 156 episodes (156.0 cycles). mean_score_over_agents 34.36149923196062 trailing 28.286529367746787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 157 episodes (157.0 cycles). mean_score_over_agents 34.82899922151118 trailing 28.550444361847827\n",
      "Finished 158 episodes (158.0 cycles). mean_score_over_agents 36.89049917543307 trailing 28.824889355713502\n",
      "Finished 159 episodes (159.0 cycles). mean_score_over_agents 35.89899919759482 trailing 29.093764349703683\n",
      "Finished 160 episodes (160.0 cycles). mean_score_over_agents 36.136999192275105 trailing 29.360954343731517\n",
      "Finished 161 episodes (161.0 cycles). mean_score_over_agents 36.25849918955937 trailing 29.60835933820158\n",
      "Finished 162 episodes (162.0 cycles). mean_score_over_agents 35.44749920768663 trailing 29.84792433284689\n",
      "Finished 163 episodes (163.0 cycles). mean_score_over_agents 35.23649921240285 trailing 30.089759327441456\n",
      "Breaking\n",
      "Reached target! mean_last_100 30.089759327441456\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = MasterAgent(num_agents, state_size=state_size, action_size=action_size, seed=0)\n",
    "session = TrainingSession(num_agents)\n",
    "scores = session.train_ppo(agent, 30.0)   # Do the training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Policy\n",
    "The code below runs the policy that has previously been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run_ppo(agent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
